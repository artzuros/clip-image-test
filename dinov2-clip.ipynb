{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gunubansal129/CS/envs/nlq/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/gunubansal129/CS/envs/nlq/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoImageProcessor\n",
    "from transformers import CLIPModel, AutoProcessor\n",
    "import faiss\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "processor_dino = AutoImageProcessor.from_pretrained('facebook/dinov2-base')\n",
    "model_dino = AutoModel.from_pretrained('facebook/dinov2-base').to(device)\n",
    "\n",
    "processor_clip = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model_clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "\n",
    "images = []\n",
    "for root, dirs, files in os.walk('/home/gunubansal129/CS/yolov8/data/images/train'):\n",
    "    for file in files:\n",
    "        if file.endswith('.jpg') or file.endswith('.png'):\n",
    "            images.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataset class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = ImageDataset(images, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_vector_to_index(embedding, index):\n",
    "    #convert embedding to numpy\n",
    "    vector = embedding.detach().cpu().numpy()\n",
    "    #Convert to float32 numpy\n",
    "    vector = np.float32(vector)\n",
    "    #Normalize vector: important to avoid wrong results when searching\n",
    "    faiss.normalize_L2(vector)\n",
    "    #Add to index\n",
    "    index.add(vector)\n",
    "\n",
    "def extract_features_dino(image):\n",
    "    with torch.no_grad():\n",
    "        inputs = processor_dino(images=image, return_tensors=\"pt\").to(device)\n",
    "        outputs = model_dino(**inputs)\n",
    "        image_features = outputs.last_hidden_state\n",
    "        return image_features.mean(dim=1)\n",
    "    \n",
    "def extract_features_clip(image):\n",
    "    with torch.no_grad():\n",
    "        inputs = processor_clip(images=image, return_tensors=\"pt\").to(device)\n",
    "        image_features = model_clip.get_image_features(**inputs)\n",
    "        return image_features\n",
    "\n",
    "index_dino = faiss.IndexFlatL2(768)\n",
    "index_clip = faiss.IndexFlatL2(512)\n",
    "\n",
    "for image in images:\n",
    "    img = Image.open(image).convert('RGB')\n",
    "    dino_features = extract_features_dino(img)\n",
    "    add_vector_to_index(dino_features, index_dino)\n",
    "    clip_features = extract_features_clip(img)\n",
    "    add_vector_to_index(clip_features, index_clip)\n",
    "\n",
    "\n",
    "faiss.write_index(index_dino, 'index_dino.index')\n",
    "faiss.write_index(index_clip, 'index_clip.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = \"/home/gunubansal129/CS/yolov8/data/images/train/ind_raja_12_2013_001_P004_78285944_29961528_C004C_16_01_2014_14_12_41_P_439.jpg\"\n",
    "img = Image.open(source).convert('RGB')\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = processor_dino(images=img, return_tensors=\"pt\").to(device)\n",
    "    outputs = model_dino(**inputs)\n",
    "    image_features = outputs.last_hidden_state\n",
    "    image_features_dino = image_features.mean(dim=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = processor_clip(images=img, return_tensors=\"pt\").to(device)\n",
    "    image_features_clip = model_clip.get_image_features(**inputs)\n",
    "\n",
    "def normalizeL2(embeddings):\n",
    "    vector = embeddings.detach().cpu().numpy()\n",
    "    vector = np.float32(vector)\n",
    "    faiss.normalize_L2(vector)\n",
    "    return vector\n",
    "\n",
    "input_features_dino = normalizeL2(image_features_dino)\n",
    "index_dino = faiss.read_index('index_dino.index')\n",
    "\n",
    "input_features_clip = normalizeL2(image_features_clip)\n",
    "index_clip = faiss.read_index('index_clip.index')\n",
    "\n",
    "D_dino, I_dino = index_dino.search(input_features_dino, 10)\n",
    "D_clip, I_clip = index_clip.search(input_features_clip, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 666 with distance 0.0\n",
      "/home/gunubansal129/CS/yolov8/data/images/train/ind_raja_12_2013_001_P004_78285944_29961528_C004C_16_01_2014_14_12_41_P_439.jpg\n",
      "Image 230 with distance 0.07882020622491837\n",
      "/home/gunubansal129/CS/yolov8/data/images/train/ind_raja_12_2013_001_P004_78285944_29961528_C004C_16_01_2014_14_12_40_P_438.jpg\n",
      "Image 189 with distance 0.08717029541730881\n",
      "/home/gunubansal129/CS/yolov8/data/images/train/ind_raja_12_2013_001_P004_78285944_29961528_C004C_16_01_2014_14_12_39_P_437.jpg\n",
      "Image 164 with distance 0.12229707092046738\n",
      "/home/gunubansal129/CS/yolov8/data/images/train/ind_raja_12_2013_001_P004_78285944_29961528_C004C_16_01_2014_14_12_27_P_433.jpg\n",
      "Image 361 with distance 0.1292014867067337\n",
      "/home/gunubansal129/CS/yolov8/data/images/train/ind_raja_12_2013_001_P004_78285944_29961528_C004C_16_01_2014_14_12_34_P_434.jpg\n"
     ]
    }
   ],
   "source": [
    "# show results\n",
    "for i in range(5):\n",
    "    print(f\"Image {I_dino[0][i]} with distance {D_dino[0][i]}\")\n",
    "    print(images[I_dino[0][i]])\n",
    "    # img = Image.open(images[I_dino[0][i]]).convert('RGB')\n",
    "    # img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 666 with distance 0.0\n",
      "/home/gunubansal129/CS/yolov8/data/images/train/ind_raja_12_2013_001_P004_78285944_29961528_C004C_16_01_2014_14_12_41_P_439.jpg\n",
      "Image 230 with distance 0.03782706335186958\n",
      "/home/gunubansal129/CS/yolov8/data/images/train/ind_raja_12_2013_001_P004_78285944_29961528_C004C_16_01_2014_14_12_40_P_438.jpg\n",
      "Image 361 with distance 0.06143786013126373\n",
      "/home/gunubansal129/CS/yolov8/data/images/train/ind_raja_12_2013_001_P004_78285944_29961528_C004C_16_01_2014_14_12_34_P_434.jpg\n",
      "Image 164 with distance 0.07154563814401627\n",
      "/home/gunubansal129/CS/yolov8/data/images/train/ind_raja_12_2013_001_P004_78285944_29961528_C004C_16_01_2014_14_12_27_P_433.jpg\n",
      "Image 1084 with distance 0.07210373878479004\n",
      "/home/gunubansal129/CS/yolov8/data/images/train/ind_raja_12_2013_001_P004_78285944_29961528_C004C_16_01_2014_14_12_36_P_436.jpg\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f\"Image {I_clip[0][i]} with distance {D_clip[0][i]}\")\n",
    "    print(images[I_clip[0][i]])\n",
    "    # img = Image.open(images[I_clip[0][i]]).convert('RGB')\n",
    "    # img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
